{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notebook Overview\n",
    "This notebook focuses on data exploration and preparation for sentiment analysis of Yelp reviews.\n",
    "\n",
    "Key objectives:\n",
    "- Data Loading and Initial Assessment\n",
    "- Distribution Analysis\n",
    "- Text Analysis\n",
    "- Preprocessing Pipeline Validation\n",
    "- Data Quality Assessment\n",
    "- Feature Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1: Initial Setup and Imports\n",
    "This cell sets up our analysis environment by importing necessary libraries and modules. We're using:\n",
    "\n",
    "- Data processing tools from a custom module\n",
    "- Sentiment analysis visualization utilities\n",
    "- Standard data science libraries (pandas, seaborn, matplotlib)\n",
    "- Custom utilities for data processing and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')  # Moving up one directory to the root\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.data_processing import DataProcessor \n",
    "from utils.dataVisualizer import DataVisualizer\n",
    "from models.sentiment_model import ModelTrainer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "2. this cell creates an instance of classes needed in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances\n",
    "dataVisualizer = DataVisualizer()\n",
    "processor = DataProcessor()\n",
    "trainer = ModelTrainer() "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "3. Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = processor.load_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Distribution Analysis\n",
    "This cell performs two key analytical tasks:\n",
    "- Analyzes and visualizes the distribution of ratings across the dataset to understand review patterns\n",
    "- Examines the sentiment distribution to identify any class imbalances in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ratings distribution\n",
    "dataVisualizer.analyze_ratings_distribution(df)\n",
    "\n",
    "# Analyze sentiment distribution on the imbalanced dataset in terms of sentiment\n",
    "dataVisualizer.analyze_sentiment_distribution(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5: Data Loading and Initialization\n",
    "\n",
    "This cell implements a critical data preparation step for our sentiment analysis pipeline. The process involves:\n",
    "\n",
    "1. Dataset Loading and Initial Processing:\n",
    "   - Loads the full Yelp Review dataset\n",
    "   - Converts the original 5-star ratings into 3 sentiment classes:\n",
    "     * Negative (0): Ratings 1-2\n",
    "     * Neutral (1): Rating 3\n",
    "     * Positive (2): Ratings 4-5\n",
    "\n",
    "2. Class Balancing:\n",
    "   - Addresses the inherent class imbalance in review datasets\n",
    "   - Implements stratified sampling to ensure equal representation\n",
    "   - Samples 5000 reviews per sentiment class (due to computational constraints)\n",
    "   - Total balanced dataset size: 15000 reviews (5000 Ã— 3 classes)\n",
    "\n",
    "3. Final Processing:\n",
    "   - Randomly shuffles the balanced dataset to prevent sequence bias\n",
    "   - Resets index for clean data handling\n",
    "   - Maintains reproducibility with fixed random state (42)\n",
    "\n",
    "This balanced approach ensures our model training won't be biased towards any particular sentiment class while keeping the dataset size manageable for efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_balanced = processor.create_balanced_dataset(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6: Balanced Dataset Analysis\n",
    "\n",
    "Exploring the characteristics of the dataset by:\n",
    "- Calculating the total number of samples in the dataset\n",
    "- Analyzing and visualizing the distribution of sentiment classes\n",
    "\n",
    "This helps to understand if thedataset is balanced or imbalanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total samples: {len(df_balanced)}\")\n",
    "dataVisualizer.analyze_sentiment_distribution(df_balanced)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7: Text Length Analysis\n",
    "This section examines the length characteristics of our text data:\n",
    "- Distribution of text lengths\n",
    "- Identify potential outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVisualizer.analyze_text_lengths(df_balanced['text'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8: Detailed Preprocessing Examples\n",
    "This section provides a detailed look at the preprocessing results by:\n",
    "- Taking 5 sample texts from the dataset\n",
    "- Showing the original and processed versions\n",
    "- Identifying sarcasm in the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = df_balanced['text'].head(5)\n",
    "\n",
    "print(\"Detailed preprocessing examples from dataset:\\n\")\n",
    "for text in sample_texts:\n",
    "    processed, is_sarcastic = processor.preprocess_text(text)\n",
    "    print(f\"Original: {text}\\n\\n\")\n",
    "    print(f\"Processed: {processed}\\n\\n\")\n",
    "    print(f\"Sarcastic: {is_sarcastic}\\n\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "9: Processing Analysis\n",
    "Here we analyze the dataset to understand:\n",
    "\n",
    "- Frequency of sarcasm in our balanced dataset\n",
    "- Presence of negations\n",
    "- Usage of special tokens\n",
    "- Frequency of URLs in the text\n",
    "\n",
    "This helps to understand the characteristics of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_texts, analysis = processor.process_batch(df['text'])\n",
    "\n",
    "print(\"Batch Analysis Statistics:\")\n",
    "print(f\"Sarcasm detected: {analysis['sarcasm_count']}\")\n",
    "print(f\"Negations found: {analysis['negation_count']}\")\n",
    "print(f\"Special tokens: {analysis['special_tokens_count']}\")\n",
    "print(f\"URLs found: {analysis['url_count']}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "10: Data Quality Assessment\n",
    "This section performs crucial data quality checks by identifying:\n",
    "\n",
    "- Missing values in the dataset\n",
    "- Duplicate entries\n",
    "\n",
    "This helps to ensure data is clean and ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Quality Checks:\")\n",
    "print(\"\\nNull values:\")\n",
    "print(df_balanced.isnull().sum())\n",
    "print(\"\\nDuplicate rows:\", df_balanced.duplicated().sum())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11: Token Length Analysis\n",
    "The final cell optimizes our model configuration by:\n",
    "\n",
    "- Analyzing token lengths across the dataset\n",
    "- Calculating an optimal MAX_LENGTH parameter\n",
    "- Adjusting the value for efficient GPU utilization\n",
    "\n",
    "This helps to balance between processing capacity and model coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenized dataset from ModelTrainer\n",
    "encoded_data = trainer.prepare_dataset(df_balanced['text'])\n",
    "\n",
    "# Analyze token lengths\n",
    "suggested_length = dataVisualizer.analyze_token_lengths(encoded_data)\n",
    "MAX_LENGTH = min(512, (suggested_length + 15) // 16 * 16)\n",
    "print(f\"\\nRecommended MAX_LENGTH: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "12: Word clouds visualization\n",
    "\n",
    "- Creates word clouds for each sentiment class (Negative, Neutral, Positive)\n",
    "- Prints the top 10 most common words for each sentiment class\n",
    "- Shows word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVisualizer.visualize_wordclouds(df_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataVisualizer.display_processed_reviews(df_balanced)  # Since it's a static method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the full dataset\n",
    "dataVisualizer.analyze_text_signals(df)\n",
    "\n",
    "# for the balanced dataset\n",
    "dataVisualizer.analyze_text_signals(df_balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
