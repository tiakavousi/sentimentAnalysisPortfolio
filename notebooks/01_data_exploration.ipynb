{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notebook Overview\n",
    "This notebook focuses on comprehensive data exploration and preparation for sentiment analysis of Yelp reviews.\n",
    "\n",
    "Key objectives:\n",
    "- Data Loading and Initial Assessment\n",
    "- Distribution Analysis\n",
    "- Text Analysis\n",
    "- Preprocessing Pipeline Validation\n",
    "- Data Quality Assessment\n",
    "- Feature Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1: Initial Setup and Imports\n",
    "This cell sets up our analysis environment by importing necessary libraries and modules. We're using:\n",
    "\n",
    "- Data processing tools from a custom module\n",
    "- Sentiment analysis visualization utilities\n",
    "- Standard data science libraries (pandas, seaborn, matplotlib)\n",
    "- Custom utilities for data processing and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')  # Moving up one directory to the root\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from data.data_processing import DataProcessor \n",
    "from utils.analysis import SentimentAnalysisVisualizer\n",
    "from models.sentiment_model import ModelTrainer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2. this cell creates an instance of a SentimentAnalysisVisualizer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances\n",
    "visualizer = SentimentAnalysisVisualizer()\n",
    "processor = DataProcessor()\n",
    "trainer = ModelTrainer() "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3. Performing the initial data exploration\n",
    "\n",
    "This cell loads the dataset and performs the following:\n",
    "- Creates an instance of a SentimentAnalysisVisualizer class\n",
    "- Loads the \"yelp_review_full\" dataset\n",
    "- displays the shape of the dataset\n",
    "- Prints the first few rows of the dataset\n",
    "- Provides additional information about the dataset, including:\n",
    "    > The data types of the columns  \n",
    "    > The memory usage of the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the raw dataset\n",
    "df = processor.load_data()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4. Distribution Analysis\n",
    "This cell performs two key analytical tasks:\n",
    "- Analyzes and visualizes the distribution of ratings across the dataset to understand review patterns\n",
    "- Examines the sentiment distribution to identify any class imbalances in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze ratings distribution\n",
    "visualizer.analyze_ratings_distribution(df)\n",
    "\n",
    "# Analyze sentiment distribution on the imbalanced dataset in terms of sentiment\n",
    "visualizer.analyze_sentiment_distribution(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5: Data Loading and Initialization\n",
    "\n",
    "This cell implements a critical data preparation step for our sentiment analysis pipeline. The process involves:\n",
    "\n",
    "1. Dataset Loading and Initial Processing:\n",
    "   - Loads the full Yelp Review dataset\n",
    "   - Converts the original 5-star ratings into 3 sentiment classes:\n",
    "     * Negative (0): Ratings 1-2\n",
    "     * Neutral (1): Rating 3\n",
    "     * Positive (2): Ratings 4-5\n",
    "\n",
    "2. Class Balancing:\n",
    "   - Addresses the inherent class imbalance in review datasets\n",
    "   - Implements stratified sampling to ensure equal representation\n",
    "   - Samples 2000 reviews per sentiment class (due to computational constraints)\n",
    "   - Total balanced dataset size: 6000 reviews (2000 Ã— 3 classes)\n",
    "\n",
    "3. Final Processing:\n",
    "   - Randomly shuffles the balanced dataset to prevent sequence bias\n",
    "   - Resets index for clean data handling\n",
    "   - Maintains reproducibility with fixed random state (42)\n",
    "\n",
    "This balanced approach ensures our model training won't be biased towards any particular sentiment class while keeping the dataset size manageable for efficient processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "df_balanced = processor.create_balanced_dataset(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6: Balanced Dataset Analysis\n",
    "\n",
    "Here we explore the fundamental characteristics of our dataset by:\n",
    "- Calculating the total number of samples in the dataset\n",
    "- Analyzing and visualizing the distribution of sentiment classes\n",
    "\n",
    "This helps us understand if we have a balanced or imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Total samples: {len(df_balanced)}\")\n",
    "visualizer.analyze_sentiment_distribution(df_balanced)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7: Text Length Analysis\n",
    "This section examines the length characteristics of our text data:\n",
    "- Distribution of text lengths\n",
    "- Identify potential outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.analyze_text_lengths(df_balanced['text'])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "8: Detailed Preprocessing Examples\n",
    "This section provides a detailed look at the preprocessing results by:\n",
    "\n",
    "Taking 5 sample texts from the dataset\n",
    "Showing the original and processed versions\n",
    "Identifying sarcasm in the texts\n",
    "This gives us concrete examples of how our preprocessing pipeline transforms real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_texts = df_balanced['text'].head(5)\n",
    "\n",
    "print(\"Detailed preprocessing examples from dataset:\\n\")\n",
    "for text in sample_texts:\n",
    "    processed, is_sarcastic = processor.preprocess_text(text)\n",
    "    print(f\"Original: {text}\\n\\n\")\n",
    "    print(f\"Processed: {processed}\\n\\n\")\n",
    "    print(f\"Sarcastic: {is_sarcastic}\\n\\n\")\n",
    "    print(\"-\" * 80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "9: Batch Processing Analysis\n",
    "Here we analyze a larger batch of 100 texts to understand:\n",
    "\n",
    "Frequency of sarcasm in our dataset\n",
    "Presence of negations\n",
    "Usage of special tokens\n",
    "Frequency of URLs in the text\n",
    "This helps us understand the complexity and characteristics of our text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = df_balanced['text'].head(100)\n",
    "processed_texts, analysis = processor.process_batch(sample_batch)\n",
    "\n",
    "print(\"Batch Analysis Statistics:\")\n",
    "print(f\"Sarcasm detected: {analysis['sarcasm_count']}\")\n",
    "print(f\"Negations found: {analysis['negation_count']}\")\n",
    "print(f\"Special tokens: {analysis['special_tokens_count']}\")\n",
    "print(f\"URLs found: {analysis['url_count']}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "10: Data Quality Assessment\n",
    "This section performs crucial data quality checks by identifying:\n",
    "\n",
    "Missing values in the dataset\n",
    "Duplicate entries\n",
    "This helps ensure our data is clean and ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Quality Checks:\")\n",
    "print(\"\\nNull values:\")\n",
    "print(df_balanced.isnull().sum())\n",
    "print(\"\\nDuplicate rows:\", df_balanced.duplicated().sum())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "11: Token Length Analysis\n",
    "The final cell optimizes our model configuration by:\n",
    "\n",
    "Analyzing token lengths across the dataset\n",
    "Calculating an optimal MAX_LENGTH parameter\n",
    "Adjusting the value for efficient GPU utilization\n",
    "This helps us balance between processing capacity and model coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tokenized dataset from ModelTrainer\n",
    "encoded_data = trainer.prepare_dataset(df_balanced['text'])\n",
    "\n",
    "# Analyze token lengths\n",
    "suggested_length = SentimentAnalysisVisualizer.analyze_token_lengths(encoded_data)\n",
    "MAX_LENGTH = min(512, (suggested_length + 15) // 16 * 16)\n",
    "print(f\"\\nRecommended MAX_LENGTH: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "12: Word clouds visualization\n",
    "\n",
    "- Creates word clouds for each sentiment class (Negative, Neutral, Positive)\n",
    "- Prints the top 10 most common words for each sentiment class\n",
    "- Shows word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer.visualize_sentiment_wordclouds(df_balanced)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
