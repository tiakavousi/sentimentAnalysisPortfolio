{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "This notebook focuses on data exploration and preparation for sentiment analysis of Yelp reviews.\n",
    "\n",
    "## Key objectives:\n",
    "- Data Loading and Initial Assessment\n",
    "- Distribution Analysis\n",
    "- Text Analysis\n",
    "- Preprocessing Pipeline Validation\n",
    "- Data Quality Assessment\n",
    "- Feature Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "This cell imports necessary libraries and modules for data processing, visualization, and model training.\n",
    "The ``` os.chdir()``` command changes the working directory to the root for relative path consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')  # Moving up one directory to the root\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from data.data_processing import DataProcessor, TextSignals, SarcasmAugmenter\n",
    "from utils.dataVisualizer import DataVisualizer\n",
    "from models.sentiment_model import ModelTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Initialization\n",
    "This cell initializes instances of the following classes :\n",
    "- DataProcessor : for handling data preprocessing\n",
    "- DataVisualizer : for handling data visualization\n",
    "- ModelTrainer : for handling training tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataProcessor = DataProcessor()\n",
    "dataVisualizer = DataVisualizer(data_processor=dataProcessor)\n",
    "trainer = ModelTrainer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Initial Analysis\n",
    "The dataset is loaded, and initial distribution analyses for ratings and sentiments are performed.\n",
    "This step helps understand the structure and balance of the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataProcessor.load_data()\n",
    "print(\"\\nInitial Distribution Analysis:\")\n",
    "dataVisualizer.analyze_ratings_distribution(df)\n",
    "dataVisualizer.analyze_sentiment_distribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation\n",
    "This cell prepares a balanced dataset by stratifying classes and splitting the data into training,\n",
    "validation, and testing sets. Model inputs are also generated for subsequent processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataProcessor.prepare_data()\n",
    "train_df = data['dataframes']['train']\n",
    "val_df = data['dataframes']['val']\n",
    "test_df = data['dataframes']['test']\n",
    "model_inputs = data['model_inputs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Distribution Analysis\n",
    "Sentiment and sarcasm distributions are analyzed across training, validation, and testing splits.\n",
    "This ensures consistent representation of different classes in each split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "for split_name, split_df in [('Training', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n{split_name} Set Analysis:\")\n",
    "    print(f\"Total samples: {len(split_df)}\")\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(split_df['sentiment'].value_counts().sort_index())\n",
    "    print(\"\\nSarcasm Distribution:\")\n",
    "    print(split_df['is_sarcastic'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Length Analysis\n",
    "Text lengths in each dataset split are analyzed to identify variations and patterns.\n",
    "This step is crucial for defining suitable input length constraints for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nText Length Analysis Across Splits:\")\n",
    "for split_name, split_df in [('Training', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n{split_name} Set Text Lengths:\")\n",
    "    dataVisualizer.analyze_text_lengths(split_df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token Length Analysis and MAX_LENGTH Recommendation\n",
    "Tokenized data lengths are analyzed to determine a recommended MAX_LENGTH value for input truncation.\n",
    "The value is adjusted to align with common model input size requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_data = trainer.prepare_dataset(train_df['text'])\n",
    "suggested_length = dataVisualizer.analyze_token_lengths(encoded_data)\n",
    "MAX_LENGTH = min(512, (suggested_length + 15) // 16 * 16)\n",
    "print(f\"\\nRecommended MAX_LENGTH: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Distribution Visualization\n",
    "Word clouds are generated to visualize the most frequent words in the training data.\n",
    "This helps identify key terms and potential biases in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVisualizer.visualize_wordclouds(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Reviews Analysis\n",
    "This cell displays a sample of processed reviews to inspect preprocessing quality.\n",
    "It ensures that the pipeline handles text properly and removes unwanted artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVisualizer.display_processed_reviews(train_df, num_samples=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Signals Analysis\n",
    "Text signal features like word count, punctuation usage, and sentiment indicators are analyzed.\n",
    "This step helps identify patterns and anomalies in textual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nText Signals Analysis for Training Set:\")\n",
    "dataVisualizer.analyze_text_signals(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Checks\n",
    "The training, validation, and testing sets are checked for null values and duplicate rows.\n",
    "This ensures data integrity and quality before model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Quality Checks:\")\n",
    "for split_name, split_df in [('Training', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n{split_name} Set:\")\n",
    "    print(\"Null values:\")\n",
    "    print(split_df.isnull().sum())\n",
    "    print(f\"Duplicate rows: {split_df.duplicated().sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
