{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notebook Overview\n",
    "This notebook focuses on data exploration and preparation for sentiment analysis of Yelp reviews.\n",
    "\n",
    "Key objectives:\n",
    "- Data Loading and Initial Assessment\n",
    "- Distribution Analysis\n",
    "- Text Analysis\n",
    "- Preprocessing Pipeline Validation\n",
    "- Data Quality Assessment\n",
    "- Feature Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 : Imports\n",
    "import os\n",
    "os.chdir('../')  # Moving up one directory to the root\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from data.data_processing import DataProcessor, TextSignals, SarcasmAugmenter\n",
    "from utils.dataVisualizer import DataVisualizer\n",
    "from models.sentiment_model import ModelTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 : Create instances\n",
    "dataProcessor = DataProcessor()\n",
    "dataVisualizer = DataVisualizer(data_processor=dataProcessor)\n",
    "trainer = ModelTrainer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load raw dataset and analyze initial distribution\n",
    "df = dataProcessor.load_data()\n",
    "print(\"\\nInitial Distribution Analysis:\")\n",
    "dataVisualizer.analyze_ratings_distribution(df)\n",
    "dataVisualizer.analyze_sentiment_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Process and prepare balanced dataset\n",
    "data = dataProcessor.prepare_data()\n",
    "train_df = data['dataframes']['train']\n",
    "val_df = data['dataframes']['val']\n",
    "test_df = data['dataframes']['test']\n",
    "model_inputs = data['model_inputs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cell 5: Analyze data splits distributions\n",
    "for split_name, split_df in [('Training', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n{split_name} Set Analysis:\")\n",
    "    print(f\"Total samples: {len(split_df)}\")\n",
    "    print(\"\\nSentiment Distribution:\")\n",
    "    print(split_df['sentiment'].value_counts().sort_index())\n",
    "    print(\"\\nSarcasm Distribution:\")\n",
    "    print(split_df['is_sarcastic'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Text length analysis\n",
    "print(\"\\nText Length Analysis Across Splits:\")\n",
    "for split_name, split_df in [('Training', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n{split_name} Set Text Lengths:\")\n",
    "    dataVisualizer.analyze_text_lengths(split_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Token length analysis and MAX_LENGTH recommendation\n",
    "encoded_data = trainer.prepare_dataset(train_df['text'])\n",
    "suggested_length = dataVisualizer.analyze_token_lengths(encoded_data)\n",
    "MAX_LENGTH = min(512, (suggested_length + 15) // 16 * 16)\n",
    "print(f\"\\nRecommended MAX_LENGTH: {MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Visualize word distributions\n",
    "dataVisualizer.visualize_wordclouds(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Sample reviews analysis\n",
    "dataVisualizer.display_processed_reviews(train_df, num_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Text signals analysis\n",
    "print(\"\\nText Signals Analysis for Training Set:\")\n",
    "dataVisualizer.analyze_text_signals(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Data quality checks\n",
    "print(\"Data Quality Checks:\")\n",
    "for split_name, split_df in [('Training', train_df), ('Validation', val_df), ('Test', test_df)]:\n",
    "    print(f\"\\n{split_name} Set:\")\n",
    "    print(\"Null values:\")\n",
    "    print(split_df.isnull().sum())\n",
    "    print(f\"Duplicate rows: {split_df.duplicated().sum()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
