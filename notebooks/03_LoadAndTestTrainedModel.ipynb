{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d5dcd2b2-4f6f-4dcd-a912-c5f437ede47e",
   "metadata": {},
   "source": [
    "Notebook Overview\n",
    "\n",
    "key objectives:\n",
    "- Loading a pre-trained sentiment analysis model\n",
    "- Validating the model's functionality\n",
    "- Analyzing training performance through visualizations\n",
    "- Generating test cases for model evaluation\n",
    "- Quality assurance through complex review scenarios"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c464c7bf-418c-481a-974c-97387ac17e97",
   "metadata": {},
   "source": [
    "1: Library Imports\n",
    "- Importing core Python libraries for system operations (os, sys)\n",
    "- Setting up deep learning frameworks (tensorflow)\n",
    "- Loading data science tools (numpy, sklearn)\n",
    "- Importing visualization libraries (seaborn, matplotlib)\n",
    "- Including custom project modules for model architecture and analysis\n",
    "- The os.chdir('../') command ensures we're working from the project root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f99982f-c792-40a8-a784-e700df541f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')  # Moving up one directory to the root\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from models.sentiment_model import EnhancedDistilBertForSentiment\n",
    "from config.model_config import ModelConfig\n",
    "from main import SentimentAnalyzer\n",
    "from utils.analysis import SentimentAnalysisVisualizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11b00530-9feb-4380-8ad6-d42f9b2834e2",
   "metadata": {},
   "source": [
    "2. Project Path Setup\n",
    "\n",
    "This cell handles Python path configuration by:\n",
    "- Identifying the project's root directory\n",
    "- Adding it to Python's system path\n",
    "\n",
    "This ensures that Python can find and import custom modules from anywhere in the project structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69aaf66-75bb-4dee-99d2-9673ecb29ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add project root to Python path\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39598a7a-300a-40aa-a650-dc925eb4eb35",
   "metadata": {},
   "source": [
    "3: Model Loading\n",
    "This cell initializes and loads the trained model:\n",
    "- Creates a new SentimentAnalyzer instance\n",
    "- Loads the model weights from epoch 5\n",
    "- Retrieves both the model and its training history\n",
    "\n",
    "The epoch=5 parameter indicates we're loading the model state after 5 training epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6105dabd-93b6-4918-8e71-267938b6bc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize analyzer and load model\n",
    "analyzer = SentimentAnalyzer()\n",
    "model, history = analyzer.load_saved_model(epoch=5)\n",
    "\n",
    "print(\"Processing data to get test split...\")\n",
    "analyzer.process_data()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ef11b30-8b22-4159-9231-d603e51b13dc",
   "metadata": {},
   "source": [
    "4: Training Visualization\n",
    "\n",
    "This cell creates visualizations of the model's training metrics:\n",
    "- Initializes the visualization tool\n",
    "- Creates plots showing:\n",
    "    - Training and validation losses\n",
    "    - Accuracy metrics\n",
    "    - Other performance indicators\n",
    "\n",
    "This helps in understanding how the model learned over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2eca337-e53d-4d3f-b2b8-55a8aedfa2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training history\n",
    "visualizer = SentimentAnalysisVisualizer()\n",
    "visualizer.visualize_training_history(history)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a27e4c51-bda0-4984-ae1f-3d827da7fa2b",
   "metadata": {},
   "source": [
    "5: Model Testing\n",
    "This final cell demonstrates practical model usage:\n",
    "\n",
    "This provides a real-world example of the model's capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368995b7-005f-4f80-a944-d9fee234ae67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating model on test set...\")\n",
    "test_predictions = []\n",
    "print(f\"Processing {len(analyzer.test_texts)} test examples...\")\n",
    "\n",
    "for i, text in enumerate(analyzer.test_texts):\n",
    "    prediction = analyzer.predict(text)\n",
    "    test_predictions.append(prediction)\n",
    "    if (i + 1) % 100 == 0:  # Progress update every 100 examples\n",
    "        print(f\"Processed {i + 1}/{len(analyzer.test_texts)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd3db0-3e86-435f-92ed-8b42512b5979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check what format our test labels are in\n",
    "print(\"Test labels shape:\", analyzer.test_labels['sentiment'].shape)\n",
    "print(\"Sample test label:\", analyzer.test_labels['sentiment'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0fea638-5671-4904-900b-30d08df8fc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Calculate metrics - note that y_true is already in correct format\n",
    "y_true = analyzer.test_labels['sentiment']  # Already integers (0, 1, 2)\n",
    "y_pred = [np.argmax([p['sentiment']['negative'], p['sentiment']['neutral'], p['sentiment']['positive']]) \n",
    "          for p in test_predictions]\n",
    "\n",
    "# 5. Display metrics\n",
    "target_names = ['Negative', 'Neutral', 'Positive']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "\n",
    "# 6. Show confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_names,\n",
    "            yticklabels=target_names)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "# 7. Show detailed examples\n",
    "print(\"\\nDetailed Examples from Test Set:\")\n",
    "sample_indices = random.sample(range(len(analyzer.test_texts)), 5)\n",
    "for idx in sample_indices:\n",
    "    text = analyzer.test_texts[idx]\n",
    "    true_sentiment = analyzer.test_labels['sentiment'][idx]\n",
    "    pred = test_predictions[idx]\n",
    "    \n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"True Sentiment: {target_names[true_sentiment]}\")\n",
    "    print(f\"Predicted probabilities:\")\n",
    "    print(f\"- Negative: {pred['sentiment']['negative']:.3f}\")\n",
    "    print(f\"- Neutral:  {pred['sentiment']['neutral']:.3f}\")\n",
    "    print(f\"- Positive: {pred['sentiment']['positive']:.3f}\")\n",
    "    print(f\"Additional features:\")\n",
    "    print(f\"- Sarcasm detected: {pred['sarcasm']['detected']}\")\n",
    "    print(f\"- Negation detected: {pred['negation']['detected']}\")\n",
    "    print(f\"- Multipolar: {pred['multipolarity']['is_multipolar']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49053656-865c-48a7-ab8a-641cab309561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. First get the basic counts\n",
    "sarcasm_counts = {\n",
    "    'True': sum(1 for p in test_predictions if p['sarcasm']['detected']),\n",
    "    'False': sum(1 for p in test_predictions if not p['sarcasm']['detected'])\n",
    "}\n",
    "\n",
    "negation_counts = {\n",
    "    'True': sum(1 for p in test_predictions if p['negation']['detected']),\n",
    "    'False': sum(1 for p in test_predictions if not p['negation']['detected'])\n",
    "}\n",
    "\n",
    "multipolar_counts = {\n",
    "    'True': sum(1 for p in test_predictions if p['multipolarity']['is_multipolar']),\n",
    "    'False': sum(1 for p in test_predictions if not p['multipolarity']['is_multipolar'])\n",
    "}\n",
    "\n",
    "# Display counts and percentages\n",
    "total = len(test_predictions)\n",
    "\n",
    "print(\"\\nFeature Distribution in Test Set:\")\n",
    "print(\"\\nSarcasm Detection:\")\n",
    "print(f\"True:  {sarcasm_counts['True']} ({sarcasm_counts['True']/total*100:.1f}%)\")\n",
    "print(f\"False: {sarcasm_counts['False']} ({sarcasm_counts['False']/total*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nNegation Detection:\")\n",
    "print(f\"True:  {negation_counts['True']} ({negation_counts['True']/total*100:.1f}%)\")\n",
    "print(f\"False: {negation_counts['False']} ({negation_counts['False']/total*100:.1f}%)\")\n",
    "\n",
    "print(\"\\nMultipolarity Detection:\")\n",
    "print(f\"True:  {multipolar_counts['True']} ({multipolar_counts['True']/total*100:.1f}%)\")\n",
    "print(f\"False: {multipolar_counts['False']} ({multipolar_counts['False']/total*100:.1f}%)\")\n",
    "\n",
    "# 2. Then add the detailed probability analysis\n",
    "print(\"\\nDetailed Feature Analysis:\")\n",
    "print(\"\\nSarcasm Probabilities:\")\n",
    "sarcasm_probs = [p['sarcasm']['probability'] for p in test_predictions]\n",
    "print(f\"Min: {min(sarcasm_probs):.3f}\")\n",
    "print(f\"Max: {max(sarcasm_probs):.3f}\")\n",
    "print(f\"Mean: {np.mean(sarcasm_probs):.3f}\")\n",
    "\n",
    "print(\"\\nNegation Probabilities:\")\n",
    "negation_probs = [p['negation']['probability'] for p in test_predictions]\n",
    "print(f\"Min: {min(negation_probs):.3f}\")\n",
    "print(f\"Max: {max(negation_probs):.3f}\")\n",
    "print(f\"Mean: {np.mean(negation_probs):.3f}\")\n",
    "\n",
    "print(\"\\nMultipolarity Scores:\")\n",
    "polarity_scores = [p['multipolarity']['score'] for p in test_predictions]\n",
    "print(f\"Min: {min(polarity_scores):.3f}\")\n",
    "print(f\"Max: {max(polarity_scores):.3f}\")\n",
    "print(f\"Mean: {np.mean(polarity_scores):.3f}\")\n",
    "\n",
    "# Check for special tokens in processed texts\n",
    "special_tokens = {\n",
    "    'SARC': sum(1 for idx, p in enumerate(test_predictions) if '_SARC_' in analyzer.test_texts[idx]),\n",
    "    'NEG': sum(1 for idx, p in enumerate(test_predictions) if '_NEG_' in analyzer.test_texts[idx])\n",
    "}\n",
    "print(\"\\nSpecial Tokens Found:\")\n",
    "print(f\"_SARC_ tokens: {special_tokens['SARC']}\")\n",
    "print(f\"_NEG_ tokens: {special_tokens['NEG']}\")\n",
    "\n",
    "# 3. Keep your visualization code if you want it\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Sarcasm plot\n",
    "ax1.bar(['True', 'False'], [sarcasm_counts['True'], sarcasm_counts['False']])\n",
    "ax1.set_title('Sarcasm Detection')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Negation plot\n",
    "ax2.bar(['True', 'False'], [negation_counts['True'], negation_counts['False']])\n",
    "ax2.set_title('Negation Detection')\n",
    "\n",
    "# Multipolarity plot\n",
    "ax3.bar(['True', 'False'], [multipolar_counts['True'], multipolar_counts['False']])\n",
    "ax3.set_title('Multipolarity Detection')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
