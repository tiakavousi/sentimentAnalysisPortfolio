{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')  # Moving up one directory to the root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 09:11:08.769000: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-30 09:11:09.264017: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-30 09:11:09.264101: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-30 09:11:09.349085: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-30 09:11:09.512298: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "\n",
    "from main import SentimentAnalyzer\n",
    "from models.sentiment_model import EnhancedDistilBertForSentiment, ModelTrainer\n",
    "from config.model_config import ModelConfig\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing Examples:\n",
      "\n",
      "Original: Great service and amazing food!\n",
      "Processed: Great service and amazing food!\n",
      "\n",
      "Original: Terrible experience, would not recommend.\n",
      "Processed: Terrible experience, would not_NEG recommend.\n",
      "\n",
      "Original: The food was okay, but the service could be better.\n",
      "Processed: The food was okay, but the service could be better.\n",
      "\n",
      "Original: Yeah right, like that's going to work...\n",
      "Processed: Yeah right, like that is going to work ELLIPSIS  _SARC_yeah right\n",
      "\n",
      "Original: Thanks a lot... now everything is broken ðŸ™„\n",
      "Processed: Thanks a lot ELLIPSIS  now everything is broken ðŸ™„ _SARC_thanks a lot (contextual)\n",
      "\n",
      "Original: Obviously this is the best restaurant ever...\n",
      "Processed: Obviously this is the best restaurant ever ELLIPSIS \n"
     ]
    }
   ],
   "source": [
    "# Initialize analyzer\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Process and show preprocessing examples\n",
    "train_texts, val_texts, train_labels, val_labels = analyzer.process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "2024-11-30 09:11:45.096179: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:274] failed call to cuInit: UNKNOWN ERROR (34)\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "BERT Model: distilbert-base-uncased\n",
      "Learning Rate: 1e-05\n",
      "Batch Size: 32\n",
      "Max Length: 200\n"
     ]
    }
   ],
   "source": [
    "# Initialize model with your specific configuration\n",
    "# Create the analyzer instance first\n",
    "analyzer = SentimentAnalyzer()\n",
    "\n",
    "# Then initialize the model\n",
    "analyzer.initialize_model()\n",
    "analyzer.initialize_model()\n",
    "\n",
    "# Show model configuration\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"BERT Model: {ModelConfig.BERT_MODEL}\")\n",
    "print(f\"Learning Rate: {ModelConfig.LEARNING_RATE}\")\n",
    "print(f\"Batch Size: {ModelConfig.BATCH_SIZE}\")\n",
    "print(f\"Max Length: {ModelConfig.MAX_LENGTH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"enhanced_distil_bert_for_sentiment_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tf_distil_bert_model_1 (TF  multiple                  66362880  \n",
      " DistilBertModel)                                                \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirecti  multiple                  1154400   \n",
      " onal)                                                           \n",
      "                                                                 \n",
      " dense_6 (Dense)             multiple                  30048     \n",
      "                                                                 \n",
      " dense_7 (Dense)             multiple                  30048     \n",
      "                                                                 \n",
      " dense_8 (Dense)             multiple                  30048     \n",
      "                                                                 \n",
      " dense_9 (Dense)             multiple                  31040     \n",
      "                                                                 \n",
      " dense_10 (Dense)            multiple                  51360     \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            multiple                  483       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 67690307 (258.22 MB)\n",
      "Trainable params: 67690307 (258.22 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input to build the model\n",
    "dummy_input = analyzer.tokenizer(\n",
    "    \"This is a dummy text\", \n",
    "    return_tensors='tf',\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=ModelConfig.MAX_LENGTH\n",
    ")\n",
    "\n",
    "# Build the model by passing dummy input\n",
    "_ = analyzer.model(dummy_input)  # Use underscore instead of asterisk\n",
    "analyzer.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "\n",
    "history = analyzer.train()\n",
    "\n",
    "# Training visualization - using actual metrics from your model\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    \"I do not like this restaurant\",\n",
    "    \"The food wasn't very good\",\n",
    "    \"I'll never come back here\",\n",
    "    \"Yeah right, this is the best service ever...\",\n",
    "    \"Thanks a lot for ruining my evening!!!\",\n",
    "    \"Obviously this is the perfect meal... ðŸ™„\",\n",
    "]\n",
    "\n",
    "# Get predictions\n",
    "for text in test_cases:\n",
    "    results = analyzer.predict(text)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(\"Predictions:\", {\n",
    "        'negative': f\"{results['negative']:.3f}\",\n",
    "        'neutral': f\"{results['neutral']:.3f}\",\n",
    "        'positive': f\"{results['positive']:.3f}\",\n",
    "        'has_sarcasm': results['has_sarcasm'],\n",
    "        'has_negation': results['has_negation'],\n",
    "        'is_multipolar': results['is_multipolar']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Analyzing both sarcasm and negation detection\n",
    "special_cases = [\n",
    "    \"This couldn't possibly be any better... ðŸ™„\",\n",
    "    \"Thank you so much for not helping at all!!!\",\n",
    "]\n",
    "\n",
    "print(\"\\nSpecial Feature Analysis:\")\n",
    "for text in special_cases:\n",
    "    results = analyzer.predict(text)\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"Sarcasm Detected: {results['has_sarcasm']}\")\n",
    "    print(f\"Negation Detected: {results['has_negation']}\")\n",
    "    print(f\"Multipolar: {results['is_multipolar']}\")\n",
    "    print(f\"Sentiment Distribution: neg={results['negative']:.2f}, neu={results['neutral']:.2f}, pos={results['positive']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
