{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Notebook Overview\n",
    "This notebook implements the training pipeline for the enhanced DistilBERT-based sentiment analysis model with multi-task learning capabilities.\n",
    "\n",
    "Key objectives:\n",
    "- Environment Configuration\n",
    "- Model Setup\n",
    "- Data Preparation\n",
    "- Model Training\n",
    "- Model Persistence\n",
    "    - Save model weights\n",
    "    - Store model configuration\n",
    "    - Preserve training history\n",
    "    - Save architecture parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1: Library Imports and System Configuration\n",
    "\n",
    "This section sets up the necessary components for model training:\n",
    "- Sets the working directory\n",
    "- Adds the project root to system path\n",
    "- Imports custom sentiment analysis modules\n",
    "- Imports the enhanced DistilBERT model architecture\n",
    "- Imports training utilities and configuration\n",
    "\n",
    "These imports provide all the necessary tools for building and training our sentiment analysis model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 12:06:47.623981: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers package location: /opt/anaconda3/lib/python3.12/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')  # Moving up one directory to the root\n",
    "os.environ['WRAPT_DISABLE_EXTENSIONS'] = 'true'\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.getcwd())))\n",
    "from main import SentimentAnalyzer\n",
    "from config.model_config import ModelConfig\n",
    "from models.sentiment_model import EnhancedDistilBertForSentiment, ModelTrainer\n",
    "from models.modelPersistence import ModelPersistence\n",
    "from utils.dataVisualizer import DataVisualizer\n",
    "from utils.modelEvaluator import ModelEvaluator"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "3: Data Preparation\n",
    "\n",
    "This cell initializes our sentiment analyzer and prepares our training data by:\n",
    "- Creating an instance of the SentimentAnalyzer\n",
    "- Loading and splitting the training and validation texts\n",
    "- Preparing corresponding labels for both sets\n",
    "\n",
    "This ensures our data is properly organized for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2100 (70.0%)\n",
      "Validation set size: 600 (20.0%)\n",
      "Test set size: 300 (10.0%)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Data Preparation\n",
    "analyzer = SentimentAnalyzer()\n",
    "analyzer.process_data()\n",
    "\n",
    "train_texts = analyzer.train_texts\n",
    "val_texts = analyzer.val_texts\n",
    "test_texts = analyzer.test_texts\n",
    "train_labels = analyzer.train_labels\n",
    "val_labels = analyzer.val_labels\n",
    "test_labels = analyzer.test_labels"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "4: Model Configuration and Initialization\n",
    "\n",
    "This section sets up our model with proper configuration by:\n",
    "- Processing the input data\n",
    "- Initializing the model architecture\n",
    "- Displaying key model hyperparameters including:\n",
    "    > Base BERT model selection\n",
    "    > Learning rate\n",
    "    > Batch size\n",
    "    > Maximum sequence length\n",
    "\n",
    "This helps us verify our model configuration before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 2100 (70.0%)\n",
      "Validation set size: 600 (20.0%)\n",
      "Test set size: 300 (10.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Configuration:\n",
      "BERT Model: distilbert-base-uncased\n",
      "Learning Rate: 5e-06\n",
      "Batch Size: 32\n",
      "Max Length: 256\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "analyzer.process_data()\n",
    "analyzer.initialize_model()\n",
    "\n",
    "# model configuration\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"BERT Model: {ModelConfig.BERT_MODEL}\")\n",
    "print(f\"Learning Rate: {ModelConfig.LEARNING_RATE}\")\n",
    "print(f\"Batch Size: {ModelConfig.BATCH_SIZE}\")\n",
    "print(f\"Max Length: {ModelConfig.MAX_LENGTH}\")\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "5: Model Architecture Verification\n",
    "\n",
    "This cell verifies our model architecture by:\n",
    "- Creating a dummy input to test the model\n",
    "- Displaying the complete model summary\n",
    "\n",
    "This helps us confirm the model structure and parameter count before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture Summary:\n",
      "Model: \"enhanced_distil_bert_for_sentiment\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " tf_distil_bert_model (TFDi  multiple                  66362880  \n",
      " stilBertModel)                                                  \n",
      "                                                                 \n",
      " bidirectional (Bidirection  multiple                  426496    \n",
      " al)                                                             \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  4128      \n",
      "                                                                 \n",
      " multi_head_attention (Mult  multiple                  16800     \n",
      " iHeadAttention)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  4128      \n",
      "                                                                 \n",
      " multi_head_attention_1 (Mu  multiple                  16800     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  4128      \n",
      "                                                                 \n",
      " multi_head_attention_2 (Mu  multiple                  16800     \n",
      " ltiHeadAttention)                                               \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  15520     \n",
      "                                                                 \n",
      " dense_4 (Dense)             multiple                  12880     \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      " sentiment (Dense)           multiple                  243       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66880803 (255.13 MB)\n",
      "Trainable params: 66880803 (255.13 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Model Architecture Verification\n",
    "analyzer.verify_model_architecture()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "6: Model Training and Visualization\n",
    "\n",
    "This important section handles the actual training process and visualization:\n",
    "- Trains the model using the prepared datasets\n",
    "- visualize the training metrics:\n",
    "    > Multiple loss components (sentiment, sarcasm, negation, polarity)\n",
    "    > Accuracy metrics for different tasks\n",
    "    > Mean Absolute Error for polarity prediction\n",
    "\n",
    "This helps us monitor the training process and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "16/66 [======>.......................] - ETA: 13:00 - loss: 1.0967 - accuracy: 0.3516"
     ]
    }
   ],
   "source": [
    "# Cell 6: Training and Visualization\n",
    "# Train model\n",
    "history = analyzer.trainer.train(analyzer.train_dataset, analyzer.val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize evaluator\n",
    "evaluator = ModelEvaluator(model=analyzer.model, trainer=analyzer.trainer)\n",
    "\n",
    "# Run evaluation\n",
    "eval_results = evaluator.evaluate_model(\n",
    "    texts=analyzer.test_texts[:500],\n",
    "    labels=analyzer.test_labels,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Visualize results\n",
    "evaluator.visualize_results(eval_results)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "7: Save the trained Model \n",
    "\n",
    "This cell implements the model saving process by:\n",
    "- Calling the saving utility with the trained model\n",
    "- Storing the final epoch results (ModelConfig.EPOCHS)\n",
    "- Saving all configurations and weights\n",
    "\n",
    "This preserves our trained model for future use or deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Save Model\n",
    "modelPersistence = ModelPersistence()\n",
    "modelPersistence.save_model(epoch=4, history=history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
